{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "#\n",
    "#-----------------------------------------------------------------------\n",
    "# This code extracts the contents of new Kickstarter projects--projects \n",
    "# whose project end date has not yet passed \n",
    "#\n",
    "# Author - Biruk Abate\n",
    "#\n",
    "#-----------------------------------------------------------------------\n",
    "#\n",
    "#######################################################################\n",
    "\n",
    "from urllib.request import urlopen # importing webpage request\n",
    "from bs4 import BeautifulSoup  # importing beautifulsoup parsing library\n",
    "import pandas as pd  # importing panda library \n",
    "df = pd.DataFrame()   # assigning a user defined (pd) to a panda Dataframe\n",
    "\n",
    "def soupconverter(url):   #function that accepts and parses url in html format and returns soup\n",
    "    html = urlopen(url)\n",
    "    soup = BeautifulSoup(html,'lxml') #lxml format is the html display format\n",
    "    return soup\n",
    "\n",
    "def data(url):     #function that accepts url, and returns author's name, backers, fund amount, days_remaining etc..\n",
    "    soup = soupconverter(url)     #accepts and parses url in html format\n",
    "    Intro = soup.find(\"div\",{\"class\":\"col-20-24\"}).p.get_text() #the \"class\" under \"div\" tag is used to idenfify and extract element of Introduction section of webpage \n",
    "    intro = Intro.replace('\\n','') #To avoid printing out '\\n' sign  \n",
    "    \n",
    "    head = soup.title.text.replace('\\n','') # soup.title.text usually extracts the title of webpage\n",
    "    \n",
    "    author = head      #User defined author is assigned to head(title)\n",
    "    mystring= \" \"      #User defined string is declared\n",
    "    mystring = mystring + author # preassigned author is added to the empty mystring\n",
    "    value = mystring.index('by')  #In the statment contained by the \"mystring\", mystring.index locates the index of 'by' statement \n",
    "    author = mystring[value+3:-14] #Extract the statement of my string 14 digits left from the last word and 3 digits right to index of 'by'\n",
    "    \n",
    "    div2 = soup.find(\"div\",{\"class\":\"full-description\"}) # locate the class named full discription under div tag \n",
    "    about=\"\"  # declare empty about string \n",
    "    for data in div2.find_all('p'):# for all paragraphs located in div2 container find paragraphs\n",
    "        about+=data.text    # add the extracted paragraphs to the empty 'about' string\n",
    "    \n",
    "    div1= soup.find(\"div\",{\"class\":\"flex flex-column-lg mb4 mb5-sm\"}) # find class named 'flex flex ...' under div tag\n",
    "    #fund = div1.find(\"div\",\"mb2-lg\")\n",
    "    fund= div1.span.text  # find text within span tag located in div1 container\n",
    "    \n",
    "    pledge = div1.find(\"div\",\"mb2-lg\").find(\"span\",class_=\"money\").text # under div1 container, located span with class 'money' and extract its text\n",
    "    \n",
    "    back = div1.find(\"div\",\"ml5 ml0-lg mb2-lg\") # under div1 container, locate div tag with name 'm15 m10...'\n",
    "\n",
    "    backers = back.div.span.text #After extracting the container for backers, extract text\n",
    "    \n",
    "    day = div1.find(\"div\",\"ml5 ml0-lg\") #select day container within div1 container tag\n",
    "    day_remaining = day.span.text    # Within the container tag, extract the text\n",
    "    \n",
    "    url = url[:-14] +'/comments'  #select 14 digits to left last character of URL and append it to the comments\n",
    "    soup = soupconverter(url)   #Call soupconverter to parse element in HTML format\n",
    "    \n",
    "    comments = soup.find_all(\"li\",{\"class\":\"NS_comments__comment\"})  # find all the 'li' tags by class name ' NS_comments...'\n",
    "    output= \" \"  #Declare an empty string\n",
    "    for comment in comments: # for all commments in all 'li' tags by class name 'NS_comments...'\n",
    "\n",
    "        if comment.h3.a.text != author:   # This is to avoid the comments written by the author\n",
    "            statement = comment.find_all('p')   # select all sentences written by a backer\n",
    "            for p in statement:    # for all sentences in paragraph, iterate and print out sentences one at a time\n",
    "                sentence = p.text.replace('\\n','') + '~'  # The tilda (~) will allow programmers to extract successive comments \n",
    "                output= output + sentence + '  '  # add the comments in the empty output sentence \n",
    "    project_status = soup.find(\"div\",class_=\"grid-row order-2-md hide-lg mb3-md\").div.div.text  # select text under div tag by the class name 'grid-row order-2-md'\n",
    "    #end date\n",
    "    return intro, head, about,fund, backers,day_remaining,output,author,pledge,project_status #function returns the introduction, title(head), Discription page(about), comments(output), pledged money, project_status\n",
    "\n",
    "csv_data = pd.read_csv('Newprojects.csv',encoding='latin-1').fillna(0) # read from New project csv\n",
    "for i in csv_data['URL']:  # select column from csv with 'URL' title\n",
    "    if i != \"URL\":   #\n",
    "        intro, title, about,fund, backers,day_remaining,comment,author,pledge,project_status = data(i) # call data function which returns the contents of webpage\n",
    "        df =df.append({'title':title,'comment':comment,'Author':author,'About':about,'Short_Discription':intro,  #append the content to a panda file\n",
    "                       'day_remaining':day_remaining,'Fund':fund,'backers':backers,'pledge':pledge,\n",
    "                       'Results':project_status\n",
    "                       ,'URL':li},ignore_index=True)\n",
    "    \n",
    "display(df)  # display for programmer the outputs of excel sheets\n",
    "\n",
    "save=\"firstData.csv\"  # declaring the name of csv file\n",
    "df.to_csv(save)       # converting the panda 'df' dataframe to csv "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
